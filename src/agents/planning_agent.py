#Request from dataagent
#DataProfile(task_type='classification', num_samples=52, num_classes=2, data_quality='good', column_info={'text': {'type': 'object', 'unique_values': 52, 'sample_values': ["This product is absolutely amazing! Best purchase I've ever made.", 'Terrible quality. Broke after one day. Would not recommend.', 'The delivery was fast and the item works as expected.']}, 'label': {'type': 'object', 'unique_values': 2, 'sample_values': ['positive', 'negative', 'positive']}}, preprocessing_needed=['text_normalization', 'tokenization', 'remove_stop_words'], confidence=0.9)

#response from planning agent
#{'base_model': 'distilbert-base-uncased', 'task_approach': 'sequence_classification', 'training_config': {'learning_rate': 2e-05, 'num_epochs': 3, 'batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.01, 'save_steps': 1000, 'eval_steps': 500, 'logging_steps': 100}, 'preprocessing_steps': ['text_cleaning', 'tokenization'], 'evaluation_metrics': ['accuracy', 'f1'], 'expected_duration': '10-20 minutes', 'resource_requirements': {'gpu': 'optional', 'memory': '4GB'}, 'confidence': 0.7}
"""
Planning Agent - Creates optimal training strategy using LLM
Uses data profile to design the best fine-tuning approach.
"""
from typing import Dict, Any, List
from dataclasses import dataclass
import os,sys
import pdb
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


from llm.client import LLMClient
from llm.prompts import PLANNING_PROMPT
#from .data_agent import DataProfile
from .data_agent import DataProfile


@dataclass
class TrainingPlan:
    """Complete training strategy generated by the planning agent."""
    base_model: str             # "distilbert-base-uncased"
    task_approach: str          # "sequence_classification", "text_generation"
    training_config: Dict       # Learning rate, epochs, batch size, etc.
    preprocessing_steps: List   # Data preparation steps
    evaluation_metrics: List    # Metrics to track during training
    expected_duration: str      # Estimated training time
    resource_requirements: Dict # GPU, memory requirements
    confidence: float           # Agent's confidence in this plan


class PlanningAgent:
    """
    Agent that creates optimal training strategies using LLM intelligence.
    
    Takes data profile and user requirements to generate:
    - Best model selection
    - Optimal hyperparameters
    - Training pipeline configuration
    """
    
    def __init__(self):
        self.llm = LLMClient()
        self.name = "PlanningAgent"
        
        # Knowledge base of available models
        self.model_catalog = {
            "text_classification": [
                "distilbert-base-uncased",
                "bert-base-uncased", 
                "roberta-base",
                "albert-base-v2"
            ],
            "text_generation": [
                "gpt2",
                "distilgpt2",
                "microsoft/DialoGPT-medium"
            ],
            "sentiment_analysis": [
                "cardiffnlp/twitter-roberta-base-sentiment-latest",
                "distilbert-base-uncased-finetuned-sst-2-english"
            ]
        }
    
    def create_training_plan(self, data_profile: DataProfile, 
                           user_prompt: str, 
                           constraints: Dict[str, Any] = None) -> TrainingPlan:
        """
        Create comprehensive training plan based on data analysis.
        
        Args:
            data_profile: Analysis from DataAgent
            user_prompt: Original user request
            constraints: Optional constraints (time, resources, etc.)
            
        Returns:
            TrainingPlan with complete strategy
        """
        constraints = constraints or {}
        
        # Use LLM to create intelligent training strategy
        llm_plan = self._llm_create_plan(data_profile, user_prompt, constraints)
        #pdb.set_trace()
        # Validate and enhance the plan
        validated_plan = self._validate_and_enhance_plan(llm_plan, data_profile)
        
        return TrainingPlan(
            base_model=validated_plan["base_model"],
            task_approach=validated_plan["task_approach"],
            training_config=validated_plan["training_config"],
            preprocessing_steps=validated_plan["preprocessing_steps"],
            evaluation_metrics=validated_plan["evaluation_metrics"],
            expected_duration=validated_plan["expected_duration"],
            resource_requirements=validated_plan["resource_requirements"],
            confidence=validated_plan["confidence"]
        )
    
    def _llm_create_plan(self, data_profile: DataProfile, 
                        user_prompt: str, 
                        constraints: Dict) -> Dict[str, Any]:
        """Use LLM to create intelligent training plan."""
        
        # Prepare context for LLM
        planning_context = {
            "user_request": user_prompt,
            "task_type": data_profile.task_type,
            "num_samples": data_profile.num_samples,
            "num_classes": data_profile.num_classes,
            "data_quality": data_profile.data_quality,
            "preprocessing_needed": data_profile.preprocessing_needed,
            "available_models": self._get_relevant_models(data_profile.task_type),
            "constraints": constraints
        }
        
        # Get LLM recommendation
        planning_prompt = PLANNING_PROMPT.format(**planning_context)
        llm_response = self.llm.plan(planning_prompt)
        #pdb.set_trace()
        return self._parse_plan_response(llm_response)
    
    def _get_relevant_models(self, task_type: str) -> List[str]:
        """Get list of models suitable for the task type."""
        if "classification" in task_type.lower():
            return self.model_catalog["text_classification"]
        elif "generation" in task_type.lower():
            return self.model_catalog["text_generation"]
        elif "sentiment" in task_type.lower():
            return self.model_catalog["sentiment_analysis"]
        else:
            return self.model_catalog["text_classification"]  # Default
    
    def _parse_plan_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM planning response into structured plan."""
        import json
        
        try:
            parsed = json.loads(response)
            
            return {
                "base_model": parsed.get("base_model", "distilbert-base-uncased"),
                "task_approach": parsed.get("task_approach", "sequence_classification"),
                "training_config": parsed.get("training_config", self._default_training_config()),
                "preprocessing_steps": parsed.get("preprocessing_steps", []),
                "evaluation_metrics": parsed.get("evaluation_metrics", ["accuracy", "f1"]),
                "expected_duration": parsed.get("expected_duration", "10-15 minutes"),
                "resource_requirements": parsed.get("resource_requirements", {"gpu": "optional", "memory": "4GB"}),
                "confidence": parsed.get("confidence", 0.8),
                "reasoning": parsed.get("reasoning", "")
            }
            
        except json.JSONDecodeError:
            # Fallback to safe defaults
            return self._fallback_plan()
    
    def _default_training_config(self) -> Dict[str, Any]:
        """Default training configuration."""
        return {
            "learning_rate": 2e-5,
            "num_epochs": 3,
            "batch_size": 16,
            "warmup_steps": 500,
            "weight_decay": 0.01,
            "save_steps": 1000,
            "eval_steps": 500,
            "logging_steps": 100
        }
    
    def _fallback_plan(self) -> Dict[str, Any]:
        """Safe fallback plan when LLM parsing fails."""
        return {
            "base_model": "distilbert-base-uncased",
            "task_approach": "sequence_classification",
            "training_config": self._default_training_config(),
            "preprocessing_steps": ["text_cleaning", "tokenization"],
            "evaluation_metrics": ["accuracy", "f1"],
            "expected_duration": "10-15 minutes",
            "resource_requirements": {"gpu": "optional", "memory": "4GB"},
            "confidence": 0.7
        }
    
    def _validate_and_enhance_plan(self, plan: Dict, data_profile: DataProfile) -> Dict[str, Any]:
        """Validate and enhance the LLM-generated plan."""
        
        # Adjust batch size based on data size
        if data_profile.num_samples < 1000:
            plan["training_config"]["batch_size"] = 8
            plan["training_config"]["num_epochs"] = 5
        elif data_profile.num_samples > 10000:
            plan["training_config"]["batch_size"] = 32
            plan["training_config"]["num_epochs"] = 2
        
        # Adjust model based on data size and quality
        if data_profile.num_samples < 500:
            # Use smaller model for small datasets
            if plan["base_model"] == "bert-base-uncased":
                plan["base_model"] = "distilbert-base-uncased"
        
        # Add preprocessing based on data quality
        if data_profile.data_quality == "needs_cleaning":
            if "text_cleaning" not in plan["preprocessing_steps"]:
                plan["preprocessing_steps"].insert(0, "text_cleaning")
        
        # Update time estimates based on final configuration
        plan["expected_duration"] = self._estimate_training_time(plan, data_profile)
        #pdb.set_trace()
        return plan
    
    def _estimate_training_time(self, plan: Dict, data_profile: DataProfile) -> str:
        """Estimate training time based on configuration."""
        
        # Simple time estimation formula
        samples = data_profile.num_samples
        epochs = plan["training_config"]["num_epochs"]
        batch_size = plan["training_config"]["batch_size"]
        
        # Rough estimates (minutes)
        steps_per_epoch = samples // batch_size
        total_steps = steps_per_epoch * epochs
        
        if total_steps < 100:
            return "5-10 minutes"
        elif total_steps < 500:
            return "10-20 minutes"
        elif total_steps < 1000:
            return "20-30 minutes"
        else:
            return "30+ minutes"
    
    # def optimize_for_constraints(self, plan: TrainingPlan, 
    #                             constraints: Dict[str, Any]) -> TrainingPlan:
    #     """Optimize training plan for specific constraints."""
        
    #     if constraints.get("max_time_minutes"):
    #         max_time = constraints["max_time_minutes"]
            
    #         if max_time < 10:
    #             # Very fast training
    #             plan.training_config["num_epochs"] = 1
    #             plan.training_config["batch_size"] = 32
    #         elif max_time < 20:
    #             # Fast training
    #             plan.training_config["num_epochs"] = 2
    #             plan.training_config["batch_size"] = 24
        
    #     if constraints.get("gpu_available") is False:
    #         # CPU-only training
    #         plan.base_model = "distilbert-base-uncased"  # Smaller model
    #         plan.training_config["batch_size"] = 8
    #         plan.resource_requirements["gpu"] = "not_required"
        
    #     return plan


# Example usage
# if __name__ == "__main__":
    
#     agent = PlanningAgent()
    
#     # Sample data profile
#     profile = DataProfile(
#         task_type="classification",
#         num_samples=2000,
#         num_classes=3,
#         data_quality="good",
#         column_info={},
#         preprocessing_needed=["text_cleaning"],
#         confidence=0.9
#     )
    
#     # Create training plan
#     plan = agent.create_training_plan(
#         data_profile=profile,
#         user_prompt="Classify customer reviews as positive or negative",
#         constraints={"max_time_minutes": 15}
#     )
    
#     print(f"Base model: {plan.base_model}")
#     print(f"Task approach: {plan.task_approach}")
#     print(f"Expected duration: {plan.expected_duration}")
#     print(f"Training config: {plan.training_config}")
#     print(f"Confidence: {plan.confidence}")

    